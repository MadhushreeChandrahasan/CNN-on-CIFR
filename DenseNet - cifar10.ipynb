{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 65
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3253,
     "status": "ok",
     "timestamp": 1577238898947,
     "user": {
      "displayName": "madhu shree",
      "photoUrl": "",
      "userId": "06119915650318419449"
     },
     "user_tz": -330
    },
    "id": "wVIx_KIigxPV",
    "outputId": "11116ff5-88a5-4048-bfe1-a0e7a1fe2110"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import keras\n",
    "# from keras.datasets import cifar10\n",
    "# from keras.models import Model, Sequential\n",
    "# from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
    "# from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "# from keras.layers import Concatenate\n",
    "# from keras.optimizers import Adam\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UNHw6luQg3gc"
   },
   "outputs": [],
   "source": [
    "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
    "# backend\n",
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7300,
     "status": "ok",
     "timestamp": 1577239285321,
     "user": {
      "displayName": "madhu shree",
      "photoUrl": "",
      "userId": "06119915650318419449"
     },
     "user_tz": -330
    },
    "id": "ApNua5lMidXb",
    "outputId": "5ec1e47b-b2bc-48d8-a146-b2390d0aec34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 4s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "# Load CIFAR10 Data\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "img_height, img_width, channel = X_train.shape[1],X_train.shape[2],X_train.shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 906,
     "status": "ok",
     "timestamp": 1577239287201,
     "user": {
      "displayName": "madhu shree",
      "photoUrl": "",
      "userId": "06119915650318419449"
     },
     "user_tz": -330
    },
    "id": "n58e8y0Qijj5",
    "outputId": "406de02c-215c-401e-932a-0f7eb3a6dacb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "y_train shape: (50000, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255\n",
    "print('x_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "print('y_train shape:', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K2o91bKyipjW"
   },
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "# convert to one hot encoing \n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 736,
     "status": "ok",
     "timestamp": 1577239290675,
     "user": {
      "displayName": "madhu shree",
      "photoUrl": "",
      "userId": "06119915650318419449"
     },
     "user_tz": -330
    },
    "id": "x3mvNwUkisnY",
    "outputId": "1da84682-6c2e-42b1-8ec0-0e9cc7aaccba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BaJxmPG5iyH2"
   },
   "outputs": [],
   "source": [
    "from keras import models, layers\n",
    "from keras.models import Model\n",
    "from keras.layers import BatchNormalization, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yp2D1VNtjUqd"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_classes = 10\n",
    "epochs = 200\n",
    "l = 6\n",
    "num_filter = 35\n",
    "compression = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ee-sge5Kg7vr"
   },
   "outputs": [],
   "source": [
    "# Dense Block\n",
    "def denseblock(input, num_filter = 12):\n",
    "    global compression\n",
    "    temp = input\n",
    "    for _ in range(l): \n",
    "        BatchNorm = layers.BatchNormalization()(temp)\n",
    "        relu = layers.Activation('relu')(BatchNorm)\n",
    "        Conv2D_3_3 = layers.Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
    "      \n",
    "        concat = layers.Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
    "        \n",
    "        temp = concat\n",
    "        \n",
    "    return temp\n",
    "\n",
    "## transition Blosck\n",
    "def transition(input, num_filter = 12):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    Conv2D_BottleNeck = layers.Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
    "    \n",
    "    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
    "    return avg\n",
    "\n",
    "#output layer\n",
    "def output_layer(input):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    AvgPooling = layers.AveragePooling2D(pool_size=(2,2))(relu)\n",
    "    c=layers.Conv2D(10, (1, 1), padding='valid')(AvgPooling)\n",
    "\n",
    "    avg=layers.GlobalAveragePooling2D()(c)\n",
    "    output=layers.Activation('softmax')(avg)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 474
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10962,
     "status": "ok",
     "timestamp": 1577239307276,
     "user": {
      "displayName": "madhu shree",
      "photoUrl": "",
      "userId": "06119915650318419449"
     },
     "user_tz": -330
    },
    "id": "anPCpQWhhGb7",
    "outputId": "b0197a68-8515-4bfc-f803-80e4cac7ff20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = layers.Input(shape=(img_height, img_width, channel,))\n",
    "First_Conv2D = layers.Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
    "\n",
    "First_Block = denseblock(First_Conv2D, num_filter)\n",
    "First_Transition = transition(First_Block, num_filter)\n",
    "\n",
    "Second_Block = denseblock(First_Transition, num_filter)\n",
    "Second_Transition = transition(Second_Block, num_filter)\n",
    "\n",
    "Third_Block = denseblock(Second_Transition, num_filter)\n",
    "Third_Transition = transition(Third_Block, num_filter)\n",
    "\n",
    "Last_Block = denseblock(Third_Transition,  num_filter)\n",
    "output = output_layer(Last_Block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 698,
     "status": "ok",
     "timestamp": 1577239309383,
     "user": {
      "displayName": "madhu shree",
      "photoUrl": "",
      "userId": "06119915650318419449"
     },
     "user_tz": -330
    },
    "id": "1kFh7pdxhNtT",
    "outputId": "3d1d92c4-d7af-464b-bd88-1188d74dfaa7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 35)   945         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 35)   140         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 35)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 35)   11025       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 70)   0           conv2d_1[0][0]                   \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 70)   280         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 70)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 35)   22050       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 105)  0           concatenate_1[0][0]              \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 105)  420         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 105)  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 35)   33075       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 32, 32, 140)  0           concatenate_2[0][0]              \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 140)  560         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 140)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 35)   44100       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 32, 32, 175)  0           concatenate_3[0][0]              \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 175)  700         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 175)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 35)   55125       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 32, 32, 210)  0           concatenate_4[0][0]              \n",
      "                                                                 conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 210)  840         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 210)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 35)   66150       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 32, 32, 245)  0           concatenate_5[0][0]              \n",
      "                                                                 conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 245)  980         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 245)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 35)   8575        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 16, 16, 35)   0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 35)   140         average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 35)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 35)   11025       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 16, 16, 70)   0           average_pooling2d_1[0][0]        \n",
      "                                                                 conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 70)   280         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 70)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 35)   22050       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 16, 16, 105)  0           concatenate_7[0][0]              \n",
      "                                                                 conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 105)  420         concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 105)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 35)   33075       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 16, 16, 140)  0           concatenate_8[0][0]              \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 140)  560         concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 140)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 35)   44100       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 16, 16, 175)  0           concatenate_9[0][0]              \n",
      "                                                                 conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 175)  700         concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 175)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 35)   55125       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 16, 16, 210)  0           concatenate_10[0][0]             \n",
      "                                                                 conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 210)  840         concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 210)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 35)   66150       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 16, 16, 245)  0           concatenate_11[0][0]             \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16, 16, 245)  980         concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 16, 16, 245)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 16, 16, 35)   8575        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 8, 8, 35)     0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 8, 35)     140         average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 35)     0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 35)     11025       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 8, 8, 70)     0           average_pooling2d_2[0][0]        \n",
      "                                                                 conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 70)     280         concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 70)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 8, 8, 35)     22050       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 8, 8, 105)    0           concatenate_13[0][0]             \n",
      "                                                                 conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 105)    420         concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 105)    0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 8, 8, 35)     33075       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 8, 8, 140)    0           concatenate_14[0][0]             \n",
      "                                                                 conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 140)    560         concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 140)    0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 8, 8, 35)     44100       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 8, 8, 175)    0           concatenate_15[0][0]             \n",
      "                                                                 conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8, 8, 175)    700         concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 175)    0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 8, 8, 35)     55125       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 8, 8, 210)    0           concatenate_16[0][0]             \n",
      "                                                                 conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 8, 8, 210)    840         concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 8, 8, 210)    0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 8, 8, 35)     66150       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 8, 8, 245)    0           concatenate_17[0][0]             \n",
      "                                                                 conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 8, 8, 245)    980         concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 8, 8, 245)    0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 8, 8, 35)     8575        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 4, 4, 35)     0           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 4, 4, 35)     140         average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 4, 4, 35)     0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 4, 4, 35)     11025       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 4, 4, 70)     0           average_pooling2d_3[0][0]        \n",
      "                                                                 conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 4, 4, 70)     280         concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 4, 4, 70)     0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 4, 4, 35)     22050       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 4, 4, 105)    0           concatenate_19[0][0]             \n",
      "                                                                 conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 4, 4, 105)    420         concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 4, 4, 105)    0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 4, 4, 35)     33075       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 4, 4, 140)    0           concatenate_20[0][0]             \n",
      "                                                                 conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 4, 4, 140)    560         concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 4, 4, 140)    0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 4, 4, 35)     44100       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 4, 4, 175)    0           concatenate_21[0][0]             \n",
      "                                                                 conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 4, 4, 175)    700         concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 4, 4, 175)    0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 4, 4, 35)     55125       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 4, 4, 210)    0           concatenate_22[0][0]             \n",
      "                                                                 conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 4, 4, 210)    840         concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 4, 4, 210)    0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 4, 4, 35)     66150       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 4, 4, 245)    0           concatenate_23[0][0]             \n",
      "                                                                 conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 4, 4, 245)    980         concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 4, 4, 245)    0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 2, 2, 245)    0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 2, 2, 10)     2460        average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 10)           0           conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 10)           0           global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 970,910\n",
      "Trainable params: 963,070\n",
      "Non-trainable params: 7,840\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[input], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 666,
     "status": "ok",
     "timestamp": 1577239311065,
     "user": {
      "displayName": "madhu shree",
      "photoUrl": "",
      "userId": "06119915650318419449"
     },
     "user_tz": -330
    },
    "id": "b4XOsW3ahSkL",
    "outputId": "099c24a4-a98c-4819-db59-d89440f86e10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# determine Loss function and Optimizer\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QGpmyzwkj5Sb"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "filepath=\"weights-improve-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DoUuLEI5kC8C"
   },
   "source": [
    "## Image Agumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "crhGk7kEhXAz",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "        \n",
    "        rotation_range=30,  # randomly rotate images in the range (deg 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally\n",
    "        height_shift_range=0.1,  # randomly shift images vertically\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        zoom_range=0.10\n",
    "        )  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14269451,
     "status": "ok",
     "timestamp": 1577253584626,
     "user": {
      "displayName": "madhu shree",
      "photoUrl": "",
      "userId": "06119915650318419449"
     },
     "user_tz": -330
    },
    "id": "VOxeL-iskKDI",
    "outputId": "0ab1427b-6f22-4951-d308-753b55cf4a3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Epoch 1/200\n",
      "781/781 [==============================] - 85s 109ms/step - loss: 1.4650 - acc: 0.4668 - val_loss: 1.7770 - val_acc: 0.4682\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.46820, saving model to weights-improve-01-0.47.hdf5\n",
      "Epoch 2/200\n",
      "781/781 [==============================] - 73s 93ms/step - loss: 1.0677 - acc: 0.6201 - val_loss: 1.2918 - val_acc: 0.5802\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.46820 to 0.58020, saving model to weights-improve-02-0.58.hdf5\n",
      "Epoch 3/200\n",
      "781/781 [==============================] - 73s 93ms/step - loss: 0.8965 - acc: 0.6861 - val_loss: 1.6210 - val_acc: 0.5194\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.58020\n",
      "Epoch 4/200\n",
      "781/781 [==============================] - 72s 93ms/step - loss: 0.7890 - acc: 0.7240 - val_loss: 1.0675 - val_acc: 0.6643\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.58020 to 0.66430, saving model to weights-improve-04-0.66.hdf5\n",
      "Epoch 5/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.7214 - acc: 0.7479 - val_loss: 0.9055 - val_acc: 0.6996\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.66430 to 0.69960, saving model to weights-improve-05-0.70.hdf5\n",
      "Epoch 6/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.6597 - acc: 0.7687 - val_loss: 0.7927 - val_acc: 0.7316\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.69960 to 0.73160, saving model to weights-improve-06-0.73.hdf5\n",
      "Epoch 7/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.6207 - acc: 0.7836 - val_loss: 0.8865 - val_acc: 0.7121\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.73160\n",
      "Epoch 8/200\n",
      "781/781 [==============================] - 69s 89ms/step - loss: 0.5835 - acc: 0.7959 - val_loss: 0.6103 - val_acc: 0.7930\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.73160 to 0.79300, saving model to weights-improve-08-0.79.hdf5\n",
      "Epoch 9/200\n",
      "781/781 [==============================] - 70s 90ms/step - loss: 0.5512 - acc: 0.8080 - val_loss: 0.7761 - val_acc: 0.7463\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.79300\n",
      "Epoch 10/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.5251 - acc: 0.8186 - val_loss: 0.7951 - val_acc: 0.7502\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.79300\n",
      "Epoch 11/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.5006 - acc: 0.8271 - val_loss: 0.8520 - val_acc: 0.7506\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.79300\n",
      "Epoch 12/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.4744 - acc: 0.8339 - val_loss: 0.6411 - val_acc: 0.7911\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.79300\n",
      "Epoch 13/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.4572 - acc: 0.8420 - val_loss: 0.5111 - val_acc: 0.8277\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.79300 to 0.82770, saving model to weights-improve-13-0.83.hdf5\n",
      "Epoch 14/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.4343 - acc: 0.8494 - val_loss: 0.6493 - val_acc: 0.7958\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.82770\n",
      "Epoch 15/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.4190 - acc: 0.8537 - val_loss: 0.5020 - val_acc: 0.8371\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.82770 to 0.83710, saving model to weights-improve-15-0.84.hdf5\n",
      "Epoch 16/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.4036 - acc: 0.8597 - val_loss: 0.5840 - val_acc: 0.8061\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.83710\n",
      "Epoch 17/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.3909 - acc: 0.8654 - val_loss: 0.6964 - val_acc: 0.7841\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.83710\n",
      "Epoch 18/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.3800 - acc: 0.8680 - val_loss: 0.5699 - val_acc: 0.8153\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.83710\n",
      "Epoch 19/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.3657 - acc: 0.8724 - val_loss: 0.4904 - val_acc: 0.8396\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.83710 to 0.83960, saving model to weights-improve-19-0.84.hdf5\n",
      "Epoch 20/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.3562 - acc: 0.8760 - val_loss: 0.5620 - val_acc: 0.8196\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.83960\n",
      "Epoch 21/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.3448 - acc: 0.8806 - val_loss: 0.5586 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.83960\n",
      "Epoch 22/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.3360 - acc: 0.8829 - val_loss: 0.6034 - val_acc: 0.8162\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.83960\n",
      "Epoch 23/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.3247 - acc: 0.8855 - val_loss: 0.4704 - val_acc: 0.8457\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.83960 to 0.84570, saving model to weights-improve-23-0.85.hdf5\n",
      "Epoch 24/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.3177 - acc: 0.8891 - val_loss: 0.4120 - val_acc: 0.8627\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.84570 to 0.86270, saving model to weights-improve-24-0.86.hdf5\n",
      "Epoch 25/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.3090 - acc: 0.8930 - val_loss: 0.5369 - val_acc: 0.8311\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.86270\n",
      "Epoch 26/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.2989 - acc: 0.8973 - val_loss: 0.4237 - val_acc: 0.8645\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.86270 to 0.86450, saving model to weights-improve-26-0.86.hdf5\n",
      "Epoch 27/200\n",
      "781/781 [==============================] - 71s 90ms/step - loss: 0.2953 - acc: 0.8952 - val_loss: 0.4423 - val_acc: 0.8588\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.86450\n",
      "Epoch 28/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.2894 - acc: 0.8978 - val_loss: 0.4422 - val_acc: 0.8608\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.86450\n",
      "Epoch 29/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.2809 - acc: 0.9022 - val_loss: 0.4562 - val_acc: 0.8596\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.86450\n",
      "Epoch 30/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.2741 - acc: 0.9039 - val_loss: 0.5129 - val_acc: 0.8501\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.86450\n",
      "Epoch 31/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.2700 - acc: 0.9062 - val_loss: 0.5362 - val_acc: 0.8367\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.86450\n",
      "Epoch 32/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.2619 - acc: 0.9079 - val_loss: 0.4369 - val_acc: 0.8658\n",
      "\n",
      "Epoch 00032: val_acc improved from 0.86450 to 0.86580, saving model to weights-improve-32-0.87.hdf5\n",
      "Epoch 33/200\n",
      "781/781 [==============================] - 73s 93ms/step - loss: 0.2594 - acc: 0.9097 - val_loss: 0.4146 - val_acc: 0.8710\n",
      "\n",
      "Epoch 00033: val_acc improved from 0.86580 to 0.87100, saving model to weights-improve-33-0.87.hdf5\n",
      "Epoch 34/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.2500 - acc: 0.9116 - val_loss: 0.4081 - val_acc: 0.8699\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.87100\n",
      "Epoch 35/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.2490 - acc: 0.9136 - val_loss: 0.4596 - val_acc: 0.8634\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.87100\n",
      "Epoch 36/200\n",
      "781/781 [==============================] - 73s 93ms/step - loss: 0.2426 - acc: 0.9136 - val_loss: 0.3783 - val_acc: 0.8778\n",
      "\n",
      "Epoch 00036: val_acc improved from 0.87100 to 0.87780, saving model to weights-improve-36-0.88.hdf5\n",
      "Epoch 37/200\n",
      "781/781 [==============================] - 72s 93ms/step - loss: 0.2346 - acc: 0.9169 - val_loss: 0.4540 - val_acc: 0.8626\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.87780\n",
      "Epoch 38/200\n",
      "781/781 [==============================] - 72s 93ms/step - loss: 0.2329 - acc: 0.9180 - val_loss: 0.4846 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.87780\n",
      "Epoch 39/200\n",
      "781/781 [==============================] - 72s 93ms/step - loss: 0.2256 - acc: 0.9210 - val_loss: 0.4827 - val_acc: 0.8561\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.87780\n",
      "Epoch 40/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.2246 - acc: 0.9203 - val_loss: 0.4176 - val_acc: 0.8712\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.87780\n",
      "Epoch 41/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.2204 - acc: 0.9218 - val_loss: 0.4657 - val_acc: 0.8689\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.87780\n",
      "Epoch 42/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.2143 - acc: 0.9247 - val_loss: 0.3917 - val_acc: 0.8779\n",
      "\n",
      "Epoch 00042: val_acc improved from 0.87780 to 0.87790, saving model to weights-improve-42-0.88.hdf5\n",
      "Epoch 43/200\n",
      "781/781 [==============================] - 70s 90ms/step - loss: 0.2140 - acc: 0.9246 - val_loss: 0.3905 - val_acc: 0.8817\n",
      "\n",
      "Epoch 00043: val_acc improved from 0.87790 to 0.88170, saving model to weights-improve-43-0.88.hdf5\n",
      "Epoch 44/200\n",
      "781/781 [==============================] - 70s 89ms/step - loss: 0.2059 - acc: 0.9270 - val_loss: 0.3617 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00044: val_acc improved from 0.88170 to 0.89220, saving model to weights-improve-44-0.89.hdf5\n",
      "Epoch 45/200\n",
      "781/781 [==============================] - 71s 90ms/step - loss: 0.2044 - acc: 0.9272 - val_loss: 0.4745 - val_acc: 0.8631\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.89220\n",
      "Epoch 46/200\n",
      "781/781 [==============================] - 70s 89ms/step - loss: 0.1988 - acc: 0.9303 - val_loss: 0.3758 - val_acc: 0.8855\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.89220\n",
      "Epoch 47/200\n",
      "781/781 [==============================] - 70s 89ms/step - loss: 0.1979 - acc: 0.9293 - val_loss: 0.4197 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.89220\n",
      "Epoch 48/200\n",
      "781/781 [==============================] - 70s 89ms/step - loss: 0.1948 - acc: 0.9312 - val_loss: 0.3385 - val_acc: 0.8962\n",
      "\n",
      "Epoch 00048: val_acc improved from 0.89220 to 0.89620, saving model to weights-improve-48-0.90.hdf5\n",
      "Epoch 49/200\n",
      "781/781 [==============================] - 70s 90ms/step - loss: 0.1891 - acc: 0.9332 - val_loss: 0.3587 - val_acc: 0.8934\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.89620\n",
      "Epoch 50/200\n",
      "781/781 [==============================] - 70s 89ms/step - loss: 0.1888 - acc: 0.9320 - val_loss: 0.5795 - val_acc: 0.8473\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.89620\n",
      "Epoch 51/200\n",
      "781/781 [==============================] - 69s 88ms/step - loss: 0.1865 - acc: 0.9333 - val_loss: 0.3761 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.89620\n",
      "Epoch 52/200\n",
      "781/781 [==============================] - 69s 89ms/step - loss: 0.1795 - acc: 0.9364 - val_loss: 0.3551 - val_acc: 0.8948\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.89620\n",
      "Epoch 53/200\n",
      "781/781 [==============================] - 70s 89ms/step - loss: 0.1771 - acc: 0.9366 - val_loss: 0.4494 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.89620\n",
      "Epoch 54/200\n",
      "781/781 [==============================] - 69s 88ms/step - loss: 0.1790 - acc: 0.9365 - val_loss: 0.3541 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.89620\n",
      "Epoch 55/200\n",
      "781/781 [==============================] - 70s 89ms/step - loss: 0.1736 - acc: 0.9389 - val_loss: 0.3756 - val_acc: 0.8896\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.89620\n",
      "Epoch 56/200\n",
      "781/781 [==============================] - 69s 88ms/step - loss: 0.1669 - acc: 0.9412 - val_loss: 0.3674 - val_acc: 0.8931\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.89620\n",
      "Epoch 57/200\n",
      "781/781 [==============================] - 70s 89ms/step - loss: 0.1684 - acc: 0.9412 - val_loss: 0.4162 - val_acc: 0.8785\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.89620\n",
      "Epoch 58/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.1648 - acc: 0.9413 - val_loss: 0.3354 - val_acc: 0.8987\n",
      "\n",
      "Epoch 00058: val_acc improved from 0.89620 to 0.89870, saving model to weights-improve-58-0.90.hdf5\n",
      "Epoch 59/200\n",
      "781/781 [==============================] - 70s 90ms/step - loss: 0.1692 - acc: 0.9396 - val_loss: 0.3796 - val_acc: 0.8892\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.89870\n",
      "Epoch 60/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.1561 - acc: 0.9447 - val_loss: 0.3981 - val_acc: 0.8839\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.89870\n",
      "Epoch 61/200\n",
      "781/781 [==============================] - 70s 90ms/step - loss: 0.1652 - acc: 0.9420 - val_loss: 0.3484 - val_acc: 0.8978\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.89870\n",
      "Epoch 62/200\n",
      "781/781 [==============================] - 69s 89ms/step - loss: 0.1544 - acc: 0.9455 - val_loss: 0.4246 - val_acc: 0.8781\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.89870\n",
      "Epoch 63/200\n",
      "781/781 [==============================] - 70s 90ms/step - loss: 0.1536 - acc: 0.9457 - val_loss: 0.4277 - val_acc: 0.8807\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.89870\n",
      "Epoch 64/200\n",
      "781/781 [==============================] - 70s 90ms/step - loss: 0.1511 - acc: 0.9478 - val_loss: 0.4667 - val_acc: 0.8726\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.89870\n",
      "Epoch 65/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.1520 - acc: 0.9457 - val_loss: 0.4085 - val_acc: 0.8843\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.89870\n",
      "Epoch 66/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.1480 - acc: 0.9483 - val_loss: 0.3839 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.89870\n",
      "Epoch 67/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.1444 - acc: 0.9480 - val_loss: 0.3756 - val_acc: 0.8914\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.89870\n",
      "Epoch 68/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.1444 - acc: 0.9486 - val_loss: 0.3299 - val_acc: 0.9022\n",
      "\n",
      "Epoch 00068: val_acc improved from 0.89870 to 0.90220, saving model to weights-improve-68-0.90.hdf5\n",
      "Epoch 69/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.1437 - acc: 0.9487 - val_loss: 0.3803 - val_acc: 0.8917\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.90220\n",
      "Epoch 70/200\n",
      "781/781 [==============================] - 70s 90ms/step - loss: 0.1427 - acc: 0.9496 - val_loss: 0.4054 - val_acc: 0.8884\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.90220\n",
      "Epoch 71/200\n",
      "781/781 [==============================] - 70s 90ms/step - loss: 0.1423 - acc: 0.9488 - val_loss: 0.3723 - val_acc: 0.8938\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.90220\n",
      "Epoch 72/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.1378 - acc: 0.9510 - val_loss: 0.3942 - val_acc: 0.8909\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.90220\n",
      "Epoch 73/200\n",
      "781/781 [==============================] - 71s 90ms/step - loss: 0.1350 - acc: 0.9513 - val_loss: 0.3606 - val_acc: 0.8959\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.90220\n",
      "Epoch 74/200\n",
      "781/781 [==============================] - 70s 90ms/step - loss: 0.1328 - acc: 0.9528 - val_loss: 0.4207 - val_acc: 0.8870\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.90220\n",
      "Epoch 75/200\n",
      "781/781 [==============================] - 70s 90ms/step - loss: 0.1338 - acc: 0.9524 - val_loss: 0.3921 - val_acc: 0.8907\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.90220\n",
      "Epoch 76/200\n",
      "781/781 [==============================] - 70s 90ms/step - loss: 0.1320 - acc: 0.9523 - val_loss: 0.3740 - val_acc: 0.8953\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.90220\n",
      "Epoch 77/200\n",
      "781/781 [==============================] - 70s 90ms/step - loss: 0.1306 - acc: 0.9542 - val_loss: 0.3914 - val_acc: 0.8917\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.90220\n",
      "Epoch 78/200\n",
      "781/781 [==============================] - 70s 90ms/step - loss: 0.1299 - acc: 0.9540 - val_loss: 0.3516 - val_acc: 0.9015\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.90220\n",
      "Epoch 79/200\n",
      "781/781 [==============================] - 70s 90ms/step - loss: 0.1249 - acc: 0.9556 - val_loss: 0.4577 - val_acc: 0.8819\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.90220\n",
      "Epoch 80/200\n",
      "781/781 [==============================] - 70s 90ms/step - loss: 0.1241 - acc: 0.9554 - val_loss: 0.3700 - val_acc: 0.8980\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.90220\n",
      "Epoch 81/200\n",
      "781/781 [==============================] - 69s 89ms/step - loss: 0.1263 - acc: 0.9553 - val_loss: 0.4214 - val_acc: 0.8823\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.90220\n",
      "Epoch 82/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.1223 - acc: 0.9563 - val_loss: 0.4953 - val_acc: 0.8748\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.90220\n",
      "Epoch 83/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.1245 - acc: 0.9555 - val_loss: 0.4062 - val_acc: 0.8916\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.90220\n",
      "Epoch 84/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.1163 - acc: 0.9589 - val_loss: 0.7051 - val_acc: 0.8495\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.90220\n",
      "Epoch 85/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.1162 - acc: 0.9592 - val_loss: 0.4217 - val_acc: 0.8888\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.90220\n",
      "Epoch 86/200\n",
      "781/781 [==============================] - 73s 93ms/step - loss: 0.1207 - acc: 0.9562 - val_loss: 0.4531 - val_acc: 0.8843\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.90220\n",
      "Epoch 87/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.1217 - acc: 0.9568 - val_loss: 0.3678 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00087: val_acc improved from 0.90220 to 0.90300, saving model to weights-improve-87-0.90.hdf5\n",
      "Epoch 88/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.1153 - acc: 0.9589 - val_loss: 0.4884 - val_acc: 0.8754\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.90300\n",
      "Epoch 89/200\n",
      "781/781 [==============================] - 72s 93ms/step - loss: 0.1126 - acc: 0.9596 - val_loss: 0.3771 - val_acc: 0.8979\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.90300\n",
      "Epoch 90/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.1135 - acc: 0.9599 - val_loss: 0.4525 - val_acc: 0.8883\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.90300\n",
      "Epoch 91/200\n",
      "781/781 [==============================] - 73s 94ms/step - loss: 0.1057 - acc: 0.9625 - val_loss: 0.4594 - val_acc: 0.8883\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.90300\n",
      "Epoch 92/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.1137 - acc: 0.9592 - val_loss: 0.5165 - val_acc: 0.8708\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.90300\n",
      "Epoch 93/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.1110 - acc: 0.9597 - val_loss: 0.3581 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.90300\n",
      "Epoch 94/200\n",
      "781/781 [==============================] - 70s 90ms/step - loss: 0.1074 - acc: 0.9626 - val_loss: 0.3684 - val_acc: 0.8990\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.90300\n",
      "Epoch 95/200\n",
      "781/781 [==============================] - 70s 90ms/step - loss: 0.1061 - acc: 0.9613 - val_loss: 0.4447 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.90300\n",
      "Epoch 96/200\n",
      "781/781 [==============================] - 70s 90ms/step - loss: 0.1067 - acc: 0.9619 - val_loss: 0.3453 - val_acc: 0.9062\n",
      "\n",
      "Epoch 00096: val_acc improved from 0.90300 to 0.90620, saving model to weights-improve-96-0.91.hdf5\n",
      "Epoch 97/200\n",
      "781/781 [==============================] - 71s 90ms/step - loss: 0.1032 - acc: 0.9643 - val_loss: 0.3922 - val_acc: 0.8991\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.90620\n",
      "Epoch 98/200\n",
      "781/781 [==============================] - 71s 90ms/step - loss: 0.1073 - acc: 0.9619 - val_loss: 0.3024 - val_acc: 0.9147\n",
      "\n",
      "Epoch 00098: val_acc improved from 0.90620 to 0.91470, saving model to weights-improve-98-0.91.hdf5\n",
      "Epoch 99/200\n",
      "781/781 [==============================] - 71s 90ms/step - loss: 0.1012 - acc: 0.9641 - val_loss: 0.4072 - val_acc: 0.8966\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.91470\n",
      "Epoch 100/200\n",
      "781/781 [==============================] - 70s 89ms/step - loss: 0.1070 - acc: 0.9618 - val_loss: 0.5053 - val_acc: 0.8758\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.91470\n",
      "Epoch 101/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.1018 - acc: 0.9641 - val_loss: 0.5229 - val_acc: 0.8795\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.91470\n",
      "Epoch 102/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.1013 - acc: 0.9639 - val_loss: 0.4354 - val_acc: 0.8909\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.91470\n",
      "Epoch 103/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.1016 - acc: 0.9639 - val_loss: 0.3521 - val_acc: 0.9054\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.91470\n",
      "Epoch 104/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.0989 - acc: 0.9652 - val_loss: 0.4028 - val_acc: 0.8941\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.91470\n",
      "Epoch 105/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.1009 - acc: 0.9639 - val_loss: 0.3638 - val_acc: 0.9032\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.91470\n",
      "Epoch 106/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.0943 - acc: 0.9677 - val_loss: 0.4666 - val_acc: 0.8862\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.91470\n",
      "Epoch 107/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.0935 - acc: 0.9662 - val_loss: 0.3219 - val_acc: 0.9137\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.91470\n",
      "Epoch 108/200\n",
      "781/781 [==============================] - 72s 93ms/step - loss: 0.0962 - acc: 0.9664 - val_loss: 0.3731 - val_acc: 0.9048\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.91470\n",
      "Epoch 109/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.0963 - acc: 0.9655 - val_loss: 0.3692 - val_acc: 0.9033\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.91470\n",
      "Epoch 110/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.0922 - acc: 0.9672 - val_loss: 0.3888 - val_acc: 0.8999\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.91470\n",
      "Epoch 111/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.0915 - acc: 0.9678 - val_loss: 0.4625 - val_acc: 0.8900\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.91470\n",
      "Epoch 112/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.0951 - acc: 0.9673 - val_loss: 0.4160 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.91470\n",
      "Epoch 113/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.0917 - acc: 0.9677 - val_loss: 0.3718 - val_acc: 0.9077\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.91470\n",
      "Epoch 114/200\n",
      "781/781 [==============================] - 73s 93ms/step - loss: 0.0881 - acc: 0.9683 - val_loss: 0.4272 - val_acc: 0.8972\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.91470\n",
      "Epoch 115/200\n",
      "781/781 [==============================] - 73s 93ms/step - loss: 0.0919 - acc: 0.9673 - val_loss: 0.4102 - val_acc: 0.8942\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.91470\n",
      "Epoch 116/200\n",
      "781/781 [==============================] - 72s 93ms/step - loss: 0.0895 - acc: 0.9690 - val_loss: 0.4212 - val_acc: 0.8978\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.91470\n",
      "Epoch 117/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.0897 - acc: 0.9681 - val_loss: 0.4187 - val_acc: 0.8966\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.91470\n",
      "Epoch 118/200\n",
      "781/781 [==============================] - 72s 93ms/step - loss: 0.0906 - acc: 0.9678 - val_loss: 0.5544 - val_acc: 0.8755\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.91470\n",
      "Epoch 119/200\n",
      "781/781 [==============================] - 74s 95ms/step - loss: 0.0849 - acc: 0.9699 - val_loss: 0.4388 - val_acc: 0.8952\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.91470\n",
      "Epoch 120/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.0883 - acc: 0.9689 - val_loss: 0.4108 - val_acc: 0.8979\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.91470\n",
      "Epoch 121/200\n",
      "781/781 [==============================] - 72s 93ms/step - loss: 0.0819 - acc: 0.9705 - val_loss: 0.4151 - val_acc: 0.9012\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.91470\n",
      "Epoch 122/200\n",
      "781/781 [==============================] - 72s 93ms/step - loss: 0.0900 - acc: 0.9675 - val_loss: 0.3731 - val_acc: 0.9024\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.91470\n",
      "Epoch 123/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.0861 - acc: 0.9697 - val_loss: 0.3805 - val_acc: 0.9087\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.91470\n",
      "Epoch 124/200\n",
      "781/781 [==============================] - 73s 93ms/step - loss: 0.0839 - acc: 0.9705 - val_loss: 0.4792 - val_acc: 0.8907\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.91470\n",
      "Epoch 125/200\n",
      "781/781 [==============================] - 71s 90ms/step - loss: 0.0844 - acc: 0.9695 - val_loss: 0.4788 - val_acc: 0.8884\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.91470\n",
      "Epoch 126/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.0804 - acc: 0.9710 - val_loss: 0.4039 - val_acc: 0.8977\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.91470\n",
      "Epoch 127/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.0850 - acc: 0.9701 - val_loss: 0.4183 - val_acc: 0.8983\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.91470\n",
      "Epoch 128/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.0821 - acc: 0.9708 - val_loss: 0.4265 - val_acc: 0.8967\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.91470\n",
      "Epoch 129/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.0796 - acc: 0.9720 - val_loss: 0.4125 - val_acc: 0.9023\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.91470\n",
      "Epoch 130/200\n",
      "781/781 [==============================] - 70s 90ms/step - loss: 0.0825 - acc: 0.9708 - val_loss: 0.4129 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.91470\n",
      "Epoch 131/200\n",
      "781/781 [==============================] - 70s 90ms/step - loss: 0.0817 - acc: 0.9711 - val_loss: 0.4421 - val_acc: 0.8947\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.91470\n",
      "Epoch 132/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.0794 - acc: 0.9715 - val_loss: 0.4449 - val_acc: 0.8956\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.91470\n",
      "Epoch 133/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.0769 - acc: 0.9722 - val_loss: 0.3812 - val_acc: 0.9031\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.91470\n",
      "Epoch 134/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.0783 - acc: 0.9721 - val_loss: 0.3701 - val_acc: 0.9090\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.91470\n",
      "Epoch 135/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.0777 - acc: 0.9723 - val_loss: 0.4446 - val_acc: 0.8966\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.91470\n",
      "Epoch 136/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.0802 - acc: 0.9720 - val_loss: 0.4079 - val_acc: 0.9019\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.91470\n",
      "Epoch 137/200\n",
      "781/781 [==============================] - 72s 93ms/step - loss: 0.0762 - acc: 0.9727 - val_loss: 0.4002 - val_acc: 0.9083\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.91470\n",
      "Epoch 138/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.0773 - acc: 0.9726 - val_loss: 0.3777 - val_acc: 0.9097\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.91470\n",
      "Epoch 139/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.0746 - acc: 0.9736 - val_loss: 0.5664 - val_acc: 0.8815\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.91470\n",
      "Epoch 140/200\n",
      "781/781 [==============================] - 70s 90ms/step - loss: 0.0764 - acc: 0.9718 - val_loss: 0.3855 - val_acc: 0.9067\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.91470\n",
      "Epoch 141/200\n",
      "781/781 [==============================] - 70s 90ms/step - loss: 0.0759 - acc: 0.9733 - val_loss: 0.3965 - val_acc: 0.9038\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.91470\n",
      "Epoch 142/200\n",
      "781/781 [==============================] - 70s 90ms/step - loss: 0.0707 - acc: 0.9748 - val_loss: 0.3958 - val_acc: 0.9021\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.91470\n",
      "Epoch 143/200\n",
      "781/781 [==============================] - 70s 89ms/step - loss: 0.0708 - acc: 0.9742 - val_loss: 0.3851 - val_acc: 0.9088\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.91470\n",
      "Epoch 144/200\n",
      "781/781 [==============================] - 69s 89ms/step - loss: 0.0749 - acc: 0.9730 - val_loss: 0.4244 - val_acc: 0.9022\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.91470\n",
      "Epoch 145/200\n",
      "781/781 [==============================] - 69s 89ms/step - loss: 0.0741 - acc: 0.9738 - val_loss: 0.4340 - val_acc: 0.9008\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.91470\n",
      "Epoch 146/200\n",
      "781/781 [==============================] - 70s 90ms/step - loss: 0.0716 - acc: 0.9745 - val_loss: 0.4244 - val_acc: 0.9021\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.91470\n",
      "Epoch 147/200\n",
      "781/781 [==============================] - 69s 88ms/step - loss: 0.0728 - acc: 0.9748 - val_loss: 0.3694 - val_acc: 0.9107\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.91470\n",
      "Epoch 148/200\n",
      "781/781 [==============================] - 70s 89ms/step - loss: 0.0715 - acc: 0.9746 - val_loss: 0.5003 - val_acc: 0.8927\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.91470\n",
      "Epoch 149/200\n",
      "781/781 [==============================] - 70s 89ms/step - loss: 0.0709 - acc: 0.9758 - val_loss: 0.3780 - val_acc: 0.9103\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.91470\n",
      "Epoch 150/200\n",
      "781/781 [==============================] - 71s 90ms/step - loss: 0.0673 - acc: 0.9760 - val_loss: 0.3599 - val_acc: 0.9088\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.91470\n",
      "Epoch 151/200\n",
      "781/781 [==============================] - 70s 90ms/step - loss: 0.0704 - acc: 0.9752 - val_loss: 0.4261 - val_acc: 0.8990\n",
      "\n",
      "Epoch 00151: val_acc did not improve from 0.91470\n",
      "Epoch 152/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.0662 - acc: 0.9771 - val_loss: 0.4348 - val_acc: 0.9012\n",
      "\n",
      "Epoch 00152: val_acc did not improve from 0.91470\n",
      "Epoch 153/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.0729 - acc: 0.9743 - val_loss: 0.3692 - val_acc: 0.9145\n",
      "\n",
      "Epoch 00153: val_acc did not improve from 0.91470\n",
      "Epoch 154/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.0691 - acc: 0.9757 - val_loss: 0.3550 - val_acc: 0.9145\n",
      "\n",
      "Epoch 00154: val_acc did not improve from 0.91470\n",
      "Epoch 155/200\n",
      "781/781 [==============================] - 72s 93ms/step - loss: 0.0673 - acc: 0.9762 - val_loss: 0.4330 - val_acc: 0.9058\n",
      "\n",
      "Epoch 00155: val_acc did not improve from 0.91470\n",
      "Epoch 156/200\n",
      "781/781 [==============================] - 73s 93ms/step - loss: 0.0712 - acc: 0.9749 - val_loss: 0.3922 - val_acc: 0.9057\n",
      "\n",
      "Epoch 00156: val_acc did not improve from 0.91470\n",
      "Epoch 157/200\n",
      "781/781 [==============================] - 73s 93ms/step - loss: 0.0693 - acc: 0.9752 - val_loss: 0.4181 - val_acc: 0.9036\n",
      "\n",
      "Epoch 00157: val_acc did not improve from 0.91470\n",
      "Epoch 158/200\n",
      "781/781 [==============================] - 73s 93ms/step - loss: 0.0662 - acc: 0.9765 - val_loss: 0.7173 - val_acc: 0.8604\n",
      "\n",
      "Epoch 00158: val_acc did not improve from 0.91470\n",
      "Epoch 159/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.0693 - acc: 0.9755 - val_loss: 0.4366 - val_acc: 0.9015\n",
      "\n",
      "Epoch 00159: val_acc did not improve from 0.91470\n",
      "Epoch 160/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.0687 - acc: 0.9759 - val_loss: 0.4352 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00160: val_acc did not improve from 0.91470\n",
      "Epoch 161/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.0678 - acc: 0.9760 - val_loss: 0.4275 - val_acc: 0.8962\n",
      "\n",
      "Epoch 00161: val_acc did not improve from 0.91470\n",
      "Epoch 162/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.0656 - acc: 0.9765 - val_loss: 0.4154 - val_acc: 0.9033\n",
      "\n",
      "Epoch 00162: val_acc did not improve from 0.91470\n",
      "Epoch 163/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.0639 - acc: 0.9778 - val_loss: 0.3790 - val_acc: 0.9103\n",
      "\n",
      "Epoch 00163: val_acc did not improve from 0.91470\n",
      "Epoch 164/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.0664 - acc: 0.9761 - val_loss: 0.4958 - val_acc: 0.8910\n",
      "\n",
      "Epoch 00164: val_acc did not improve from 0.91470\n",
      "Epoch 165/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.0649 - acc: 0.9772 - val_loss: 0.4252 - val_acc: 0.9040\n",
      "\n",
      "Epoch 00165: val_acc did not improve from 0.91470\n",
      "Epoch 166/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.0662 - acc: 0.9761 - val_loss: 0.4126 - val_acc: 0.9075\n",
      "\n",
      "Epoch 00166: val_acc did not improve from 0.91470\n",
      "Epoch 167/200\n",
      "781/781 [==============================] - 73s 93ms/step - loss: 0.0683 - acc: 0.9760 - val_loss: 0.4388 - val_acc: 0.8986\n",
      "\n",
      "Epoch 00167: val_acc did not improve from 0.91470\n",
      "Epoch 168/200\n",
      "781/781 [==============================] - 73s 93ms/step - loss: 0.0646 - acc: 0.9772 - val_loss: 0.3659 - val_acc: 0.9128\n",
      "\n",
      "Epoch 00168: val_acc did not improve from 0.91470\n",
      "Epoch 169/200\n",
      "781/781 [==============================] - 73s 93ms/step - loss: 0.0657 - acc: 0.9769 - val_loss: 0.4611 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00169: val_acc did not improve from 0.91470\n",
      "Epoch 170/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.0639 - acc: 0.9775 - val_loss: 0.4198 - val_acc: 0.9064\n",
      "\n",
      "Epoch 00170: val_acc did not improve from 0.91470\n",
      "Epoch 171/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.0642 - acc: 0.9778 - val_loss: 0.3893 - val_acc: 0.9096\n",
      "\n",
      "Epoch 00171: val_acc did not improve from 0.91470\n",
      "Epoch 172/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.0605 - acc: 0.9789 - val_loss: 0.4614 - val_acc: 0.8971\n",
      "\n",
      "Epoch 00172: val_acc did not improve from 0.91470\n",
      "Epoch 173/200\n",
      "781/781 [==============================] - 72s 93ms/step - loss: 0.0621 - acc: 0.9782 - val_loss: 0.4036 - val_acc: 0.9076\n",
      "\n",
      "Epoch 00173: val_acc did not improve from 0.91470\n",
      "Epoch 174/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.0620 - acc: 0.9778 - val_loss: 0.3759 - val_acc: 0.9149\n",
      "\n",
      "Epoch 00174: val_acc improved from 0.91470 to 0.91490, saving model to weights-improve-174-0.91.hdf5\n",
      "Epoch 175/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.0633 - acc: 0.9778 - val_loss: 0.3947 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00175: val_acc did not improve from 0.91490\n",
      "Epoch 176/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.0604 - acc: 0.9785 - val_loss: 0.3812 - val_acc: 0.9122\n",
      "\n",
      "Epoch 00176: val_acc did not improve from 0.91490\n",
      "Epoch 177/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.0601 - acc: 0.9787 - val_loss: 0.3793 - val_acc: 0.9096\n",
      "\n",
      "Epoch 00177: val_acc did not improve from 0.91490\n",
      "Epoch 178/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.0623 - acc: 0.9782 - val_loss: 0.3956 - val_acc: 0.9079\n",
      "\n",
      "Epoch 00178: val_acc did not improve from 0.91490\n",
      "Epoch 179/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.0604 - acc: 0.9787 - val_loss: 0.3929 - val_acc: 0.9112\n",
      "\n",
      "Epoch 00179: val_acc did not improve from 0.91490\n",
      "Epoch 180/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.0615 - acc: 0.9781 - val_loss: 0.4591 - val_acc: 0.8964\n",
      "\n",
      "Epoch 00180: val_acc did not improve from 0.91490\n",
      "Epoch 181/200\n",
      "781/781 [==============================] - 70s 90ms/step - loss: 0.0576 - acc: 0.9799 - val_loss: 0.5308 - val_acc: 0.8887\n",
      "\n",
      "Epoch 00181: val_acc did not improve from 0.91490\n",
      "Epoch 182/200\n",
      "781/781 [==============================] - 70s 89ms/step - loss: 0.0588 - acc: 0.9797 - val_loss: 0.4311 - val_acc: 0.9075\n",
      "\n",
      "Epoch 00182: val_acc did not improve from 0.91490\n",
      "Epoch 183/200\n",
      "781/781 [==============================] - 69s 89ms/step - loss: 0.0588 - acc: 0.9786 - val_loss: 0.4555 - val_acc: 0.8999\n",
      "\n",
      "Epoch 00183: val_acc did not improve from 0.91490\n",
      "Epoch 184/200\n",
      "781/781 [==============================] - 70s 89ms/step - loss: 0.0576 - acc: 0.9802 - val_loss: 0.4089 - val_acc: 0.9113\n",
      "\n",
      "Epoch 00184: val_acc did not improve from 0.91490\n",
      "Epoch 185/200\n",
      "781/781 [==============================] - 70s 90ms/step - loss: 0.0557 - acc: 0.9804 - val_loss: 0.4369 - val_acc: 0.9079\n",
      "\n",
      "Epoch 00185: val_acc did not improve from 0.91490\n",
      "Epoch 186/200\n",
      "781/781 [==============================] - 70s 89ms/step - loss: 0.0580 - acc: 0.9791 - val_loss: 0.4074 - val_acc: 0.9121\n",
      "\n",
      "Epoch 00186: val_acc did not improve from 0.91490\n",
      "Epoch 187/200\n",
      "781/781 [==============================] - 70s 90ms/step - loss: 0.0611 - acc: 0.9790 - val_loss: 0.3936 - val_acc: 0.9100\n",
      "\n",
      "Epoch 00187: val_acc did not improve from 0.91490\n",
      "Epoch 188/200\n",
      "781/781 [==============================] - 70s 89ms/step - loss: 0.0570 - acc: 0.9802 - val_loss: 0.4275 - val_acc: 0.9012\n",
      "\n",
      "Epoch 00188: val_acc did not improve from 0.91490\n",
      "Epoch 189/200\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.0560 - acc: 0.9803 - val_loss: 0.4029 - val_acc: 0.9101\n",
      "\n",
      "Epoch 00189: val_acc did not improve from 0.91490\n",
      "Epoch 190/200\n",
      "781/781 [==============================] - 70s 90ms/step - loss: 0.0595 - acc: 0.9791 - val_loss: 0.4394 - val_acc: 0.9056\n",
      "\n",
      "Epoch 00190: val_acc did not improve from 0.91490\n",
      "Epoch 191/200\n",
      "781/781 [==============================] - 70s 89ms/step - loss: 0.0539 - acc: 0.9807 - val_loss: 0.4131 - val_acc: 0.9057\n",
      "\n",
      "Epoch 00191: val_acc did not improve from 0.91490\n",
      "Epoch 192/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.0550 - acc: 0.9809 - val_loss: 0.3876 - val_acc: 0.9066\n",
      "\n",
      "Epoch 00192: val_acc did not improve from 0.91490\n",
      "Epoch 193/200\n",
      "781/781 [==============================] - 73s 93ms/step - loss: 0.0558 - acc: 0.9804 - val_loss: 0.3999 - val_acc: 0.9133\n",
      "\n",
      "Epoch 00193: val_acc did not improve from 0.91490\n",
      "Epoch 194/200\n",
      "781/781 [==============================] - 73s 93ms/step - loss: 0.0566 - acc: 0.9803 - val_loss: 0.3788 - val_acc: 0.9148\n",
      "\n",
      "Epoch 00194: val_acc did not improve from 0.91490\n",
      "Epoch 195/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.0530 - acc: 0.9815 - val_loss: 0.4383 - val_acc: 0.9050\n",
      "\n",
      "Epoch 00195: val_acc did not improve from 0.91490\n",
      "Epoch 196/200\n",
      "781/781 [==============================] - 73s 93ms/step - loss: 0.0563 - acc: 0.9797 - val_loss: 0.4529 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00196: val_acc did not improve from 0.91490\n",
      "Epoch 197/200\n",
      "781/781 [==============================] - 73s 93ms/step - loss: 0.0540 - acc: 0.9807 - val_loss: 0.6403 - val_acc: 0.8780\n",
      "\n",
      "Epoch 00197: val_acc did not improve from 0.91490\n",
      "Epoch 198/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.0548 - acc: 0.9802 - val_loss: 0.3998 - val_acc: 0.9092\n",
      "\n",
      "Epoch 00198: val_acc did not improve from 0.91490\n",
      "Epoch 199/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.0550 - acc: 0.9801 - val_loss: 0.4458 - val_acc: 0.9032\n",
      "\n",
      "Epoch 00199: val_acc did not improve from 0.91490\n",
      "Epoch 200/200\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.0551 - acc: 0.9810 - val_loss: 0.5177 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00200: val_acc did not improve from 0.91490\n"
     ]
    }
   ],
   "source": [
    "datagen.fit(X_train)\n",
    "\n",
    "    # fit the model on the batches generated by datagen.flow()\n",
    "history=model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "                        steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "                        validation_data=(X_test, y_test),\n",
    "                        epochs=200, verbose=1, workers=4,\n",
    "                        callbacks=callbacks_list\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2387,
     "status": "ok",
     "timestamp": 1577255945696,
     "user": {
      "displayName": "madhu shree",
      "photoUrl": "",
      "userId": "06119915650318419449"
     },
     "user_tz": -330
    },
    "id": "PPrKZkMXkTWr",
    "outputId": "e8f3a019-8df4-44db-f042-99e9df5e3313"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# https://gist.github.com/greydanus/f6eee59eaf1d90fcb3b534a25362cea4\n",
    "# https://stackoverflow.com/a/14434334\n",
    "# this function is used to update the plots for each epoch and error\n",
    "def plt_dynamic(x, vy, ty, ax, colors=['b']):\n",
    "    ax.plot(x, vy, 'b', label=\"Validation Loss\")\n",
    "    ax.plot(x, ty, 'r', label=\"Train Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n",
    "\n",
    "# list of epoch numbers\n",
    "x = list(range(1,epochs+1))\n",
    "\n",
    "\n",
    "vy = history.history['val_loss']\n",
    "ty = history.history['loss']\n",
    "plt_dynamic(x, vy, ty, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5741,
     "status": "ok",
     "timestamp": 1577255959761,
     "user": {
      "displayName": "madhu shree",
      "photoUrl": "",
      "userId": "06119915650318419449"
     },
     "user_tz": -330
    },
    "id": "ZcWydmIVhZGr",
    "outputId": "f8949a7b-9ab4-4fa2-9e20-1a2b9aaf8706"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 349us/step\n",
      "Test loss: 0.5177401971131563\n",
      "Test accuracy: 0.8935\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xZLqv93uyHrG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sE_yRBZMyLIh"
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"weights-improve-174-0.91.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3vmJkOBhyWV9"
   },
   "outputs": [],
   "source": [
    "labels = {\n",
    "    0: 'airplane',\n",
    "    1: 'automobile',\n",
    "    2: 'bird',\n",
    "    3: 'cat',\n",
    "    4: 'deer',\n",
    "    5: 'dog',\n",
    "    6: 'frog',\n",
    "    7: 'horse',\n",
    "    8: 'ship',\n",
    "    9: 'truck',\n",
    "    \n",
    "}\n",
    "\n",
    "# Utility function to print the confusion matrix\n",
    "def confusion_matrix(Y_true, Y_pred):\n",
    "    Y_true = pd.Series([labels[y] for y in np.argmax(Y_true, axis=1)])\n",
    "    Y_pred = pd.Series([labels[y] for y in np.argmax(Y_pred, axis=1)])\n",
    "\n",
    "    return pd.crosstab(Y_true, Y_pred, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4474,
     "status": "ok",
     "timestamp": 1577256077902,
     "user": {
      "displayName": "madhu shree",
      "photoUrl": "",
      "userId": "06119915650318419449"
     },
     "user_tz": -330
    },
    "id": "CmfW7U9nyapg",
    "outputId": "4164bc51-6f56-4ea3-f293-902a52eaf0af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred        airplane  automobile  bird  cat  deer  dog  frog  horse  ship  \\\n",
      "True                                                                        \n",
      "airplane         897           6    19    4     5    2     7      1    43   \n",
      "automobile         1         970     1    0     0    0     0      1     5   \n",
      "bird              10           0   918   14    24    5    17      8     3   \n",
      "cat                8           8    36  791    34   62    39      8     5   \n",
      "deer               3           0    21   12   940    5     9      8     0   \n",
      "dog                6           1    22   84    25  824    16     18     2   \n",
      "frog               4           0    18    8     3    0   965      0     0   \n",
      "horse              3           1    12    7    25   12     4    933     1   \n",
      "ship              12           6     2    1     0    0     1      1   969   \n",
      "truck              4          34     1    3     1    0     3      1    11   \n",
      "\n",
      "Pred        truck  \n",
      "True               \n",
      "airplane       16  \n",
      "automobile     22  \n",
      "bird            1  \n",
      "cat             9  \n",
      "deer            2  \n",
      "dog             2  \n",
      "frog            2  \n",
      "horse           2  \n",
      "ship            8  \n",
      "truck         942  \n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', 10)\n",
    "print(confusion_matrix(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4014,
     "status": "ok",
     "timestamp": 1577256103485,
     "user": {
      "displayName": "madhu shree",
      "photoUrl": "",
      "userId": "06119915650318419449"
     },
     "user_tz": -330
    },
    "id": "Kx2LwZvoyfid",
    "outputId": "7a55faba-4949-464f-8c20-a490558638be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 332us/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 771,
     "status": "ok",
     "timestamp": 1577256115144,
     "user": {
      "displayName": "madhu shree",
      "photoUrl": "",
      "userId": "06119915650318419449"
     },
     "user_tz": -330
    },
    "id": "8S6dsFl8ykCU",
    "outputId": "334b8891-866b-4c66-9d14-72cc2d0fd44e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.37590156150981785, 0.9149]"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation:\n",
    "\n",
    "1.After training 200 epochs we achieved test accuracy of 91.49%."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of DenseNet - cifar10.ipynb",
   "provenance": [
    {
     "file_id": "1NGQjke72AS93IOpNcnE9diQEg78-sU3C",
     "timestamp": 1577030315872
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
